{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from natsort import natsorted\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIV2K_path = \"/data/whq/data/DIV2K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.RandomCrop(128)\n",
    "])\n",
    "transform_A =A.Compose([\n",
    "    A.RandomCrop(400,400),\n",
    "    A.augmentations.transforms.ChannelShuffle(0.5),\n",
    "    ToTensorV2()\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**使用 torchvision 进行变换**\n",
    "1. 使用 PIL 进行图像读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIV2K_Dataset(Dataset):\n",
    "    def __init__(self, transforms_=None, mode='train'):\n",
    "        self.transform = transforms_\n",
    "        self.mode = mode\n",
    "        if mode == 'train':\n",
    "            self.files = natsorted(sorted(glob.glob(DIV2K_path+\"/train\"+\"/*.\"+\"png\")))\n",
    "        else:\n",
    "            self.files = natsorted(\n",
    "                sorted(glob.glob(DIV2K_path+\"/valid\"+\"/*.\"+\"png\")))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # try:\n",
    "        image = Image.open(self.files[index]).convert(\n",
    "            'RGB')  # 使用 PIL 读取并转为 RGB\n",
    "        item = self.transform(image)\n",
    "        return item\n",
    "        # except:\n",
    "        #     return self.__getitem__(index+1)  # 这个不理解\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "DIV2K_train_loader = DataLoader(\n",
    "    DIV2K_Dataset(transforms_=transform, mode=\"train\"),\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "DIV2K_val_loader = DataLoader(\n",
    "    DIV2K_Dataset(transforms_=transform, mode=\"val\"),\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 使用 cv2 进行图像读取，并使用 albumentations 进行数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIV2K_Dataset(Dataset):\n",
    "    def __init__(self, transforms_=None, mode='train'):\n",
    "        self.transform = transforms_\n",
    "        self.mode = mode\n",
    "        if mode == 'train':\n",
    "            self.files = natsorted(\n",
    "                sorted(glob.glob(DIV2K_path+\"/DIV2K_train_HR\"+\"/*.\"+\"png\")))\n",
    "        else:\n",
    "            self.files = natsorted(\n",
    "                sorted(glob.glob(DIV2K_path+\"/DIV2K_valid_HR\"+\"/*.\"+\"png\")))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = cv2.imread(self.files[index])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # 转换为 RGB\n",
    "        trans_img = self.transform(img)\n",
    "        item= trans_img['image']\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "DIV2K_train_loader = DataLoader(\n",
    "    DIV2K_Dataset(transforms_=transform_A, mode=\"train\"),\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "DIV2K_val_loader = DataLoader(\n",
    "    DIV2K_Dataset(transforms_=transform_A, mode=\"val\"),\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**使用 albumentations 进行变换**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIV2K_Dataset(Dataset):\n",
    "    def __init__(self, transforms_=None, mode='train'):\n",
    "        self.transform = transforms_\n",
    "        self.mode = mode\n",
    "        if mode == 'train':\n",
    "            self.files = natsorted(sorted(glob.glob(DIV2K_path+\"/train\"+\"/*.\"+\"png\")))\n",
    "        else:\n",
    "            self.files = natsorted(\n",
    "                sorted(glob.glob(DIV2K_path+\"/valid\"+\"/*.\"+\"png\")))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # try:\n",
    "        image = Image.open(self.files[index]).convert(\n",
    "            'RGB')  # 使用 PIL 读取并转为 RGB\n",
    "        image = np.array(image) # 转为 np 数组\n",
    "        transformed_item = self.transform(image=image)\n",
    "        # item=T.ToTensor(transformed_item['image'])\n",
    "        return transformed_item['image']\n",
    "        # except:\n",
    "        #     return self.__getitem__(index+1)  # 这个不理解\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "DIV2K_train_loader = DataLoader(\n",
    "    DIV2K_Dataset(transforms_=transform_A, mode=\"train\"),\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "DIV2K_val_loader = DataLoader(\n",
    "    DIV2K_Dataset(transforms_=transform_A, mode=\"val\"),\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 tensorboard 来查看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mf:\\deep_learning\\dl_env\\lib\\site-packages\\PIL\\ImageFile.py:518\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 518\u001b[0m     fh \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mfileno()\n\u001b[0;32m    519\u001b[0m     fp\u001b[39m.\u001b[39mflush()\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m imgs\u001b[39m=\u001b[39mdata\n\u001b[0;32m      5\u001b[0m \u001b[39m# print(imgs.type())\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m writer\u001b[39m.\u001b[39;49madd_images(\u001b[39m\"\u001b[39;49m\u001b[39mtrain_data_droplast\u001b[39;49m\u001b[39m\"\u001b[39;49m,imgs,step)\n\u001b[0;32m      7\u001b[0m step\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(step)\n",
      "File \u001b[1;32mf:\\deep_learning\\dl_env\\lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:662\u001b[0m, in \u001b[0;36mSummaryWriter.add_images\u001b[1;34m(self, tag, img_tensor, global_step, walltime, dataformats)\u001b[0m\n\u001b[0;32m    658\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mcaffe2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m workspace\n\u001b[0;32m    660\u001b[0m     img_tensor \u001b[39m=\u001b[39m workspace\u001b[39m.\u001b[39mFetchBlob(img_tensor)\n\u001b[0;32m    661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_file_writer()\u001b[39m.\u001b[39madd_summary(\n\u001b[1;32m--> 662\u001b[0m     image(tag, img_tensor, dataformats\u001b[39m=\u001b[39;49mdataformats), global_step, walltime\n\u001b[0;32m    663\u001b[0m )\n",
      "File \u001b[1;32mf:\\deep_learning\\dl_env\\lib\\site-packages\\torch\\utils\\tensorboard\\summary.py:441\u001b[0m, in \u001b[0;36mimage\u001b[1;34m(tag, tensor, rescale, dataformats)\u001b[0m\n\u001b[0;32m    439\u001b[0m tensor \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m    440\u001b[0m tensor \u001b[39m=\u001b[39m (tensor \u001b[39m*\u001b[39m scale_factor)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8)\n\u001b[1;32m--> 441\u001b[0m image \u001b[39m=\u001b[39m make_image(tensor, rescale\u001b[39m=\u001b[39;49mrescale)\n\u001b[0;32m    442\u001b[0m \u001b[39mreturn\u001b[39;00m Summary(value\u001b[39m=\u001b[39m[Summary\u001b[39m.\u001b[39mValue(tag\u001b[39m=\u001b[39mtag, image\u001b[39m=\u001b[39mimage)])\n",
      "File \u001b[1;32mf:\\deep_learning\\dl_env\\lib\\site-packages\\torch\\utils\\tensorboard\\summary.py:490\u001b[0m, in \u001b[0;36mmake_image\u001b[1;34m(tensor, rescale, rois, labels)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mio\u001b[39;00m\n\u001b[0;32m    489\u001b[0m output \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO()\n\u001b[1;32m--> 490\u001b[0m image\u001b[39m.\u001b[39;49msave(output, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPNG\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    491\u001b[0m image_string \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    492\u001b[0m output\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mf:\\deep_learning\\dl_env\\lib\\site-packages\\PIL\\Image.py:2431\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2428\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mw+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2430\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2431\u001b[0m     save_handler(\u001b[39mself\u001b[39;49m, fp, filename)\n\u001b[0;32m   2432\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   2433\u001b[0m     \u001b[39mif\u001b[39;00m open_fp:\n",
      "File \u001b[1;32mf:\\deep_learning\\dl_env\\lib\\site-packages\\PIL\\PngImagePlugin.py:1420\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[0;32m   1418\u001b[0m     _write_multiple_frames(im, fp, chunk, rawmode, default_image, append_images)\n\u001b[0;32m   1419\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1420\u001b[0m     ImageFile\u001b[39m.\u001b[39;49m_save(im, _idat(fp, chunk), [(\u001b[39m\"\u001b[39;49m\u001b[39mzip\u001b[39;49m\u001b[39m\"\u001b[39;49m, (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m) \u001b[39m+\u001b[39;49m im\u001b[39m.\u001b[39;49msize, \u001b[39m0\u001b[39;49m, rawmode)])\n\u001b[0;32m   1422\u001b[0m \u001b[39mif\u001b[39;00m info:\n\u001b[0;32m   1423\u001b[0m     \u001b[39mfor\u001b[39;00m info_chunk \u001b[39min\u001b[39;00m info\u001b[39m.\u001b[39mchunks:\n",
      "File \u001b[1;32mf:\\deep_learning\\dl_env\\lib\\site-packages\\PIL\\ImageFile.py:522\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    520\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[0;32m    521\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAttributeError\u001b[39;00m, io\u001b[39m.\u001b[39mUnsupportedOperation) \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m--> 522\u001b[0m     _encode_tile(im, fp, tile, bufsize, \u001b[39mNone\u001b[39;49;00m, exc)\n\u001b[0;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(fp, \u001b[39m\"\u001b[39m\u001b[39mflush\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    524\u001b[0m     fp\u001b[39m.\u001b[39mflush()\n",
      "File \u001b[1;32mf:\\deep_learning\\dl_env\\lib\\site-packages\\PIL\\ImageFile.py:541\u001b[0m, in \u001b[0;36m_encode_tile\u001b[1;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[39mif\u001b[39;00m exc:\n\u001b[0;32m    539\u001b[0m     \u001b[39m# compress to Python file-compatible object\u001b[39;00m\n\u001b[0;32m    540\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 541\u001b[0m         l, s, d \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mencode(bufsize)\n\u001b[0;32m    542\u001b[0m         fp\u001b[39m.\u001b[39mwrite(d)\n\u001b[0;32m    543\u001b[0m         \u001b[39mif\u001b[39;00m s:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(\"DIV2K_train_dataloader\")\n",
    "step =0\n",
    "for data in DIV2K_train_loader:\n",
    "    imgs=data\n",
    "    # print(imgs.type())\n",
    "    writer.add_images(\"train_data_droplast\",imgs,step)\n",
    "    step+=1\n",
    "    print(step)\n",
    "\n",
    "writer.close()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 库引入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whq/data/env/Hinet_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "# Feed Forward(MLP)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# self attention\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "# transformer block\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])  # 存储 Transformer 的每一个块\n",
    "        for _ in range(depth):  # 看要堆叠多少个 Transformer blcok\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),  # self attention\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))    # mlp\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class model1_encoder(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, dim, extract_dim, depth, heads, mlp_dim, channels=3, dim_head=64, dropout=0., emb_dropout=0.):\n",
    "        \"\"\"\n",
    "            image_size: 输入图像大小\n",
    "            patch_size: patch size\n",
    "            dim: token's length\n",
    "            extract_dim: feature extractor output's dim\n",
    "            depth: numbers of transformer block\n",
    "            heads: numbers of multi-attention head\n",
    "            mlp_dim: mlp's dim\n",
    "            dim_head: one of the head's dim\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'  # 防止划分不完全\n",
    "\n",
    "        num_patches = (image_height//patch_height)*(image_width//patch_width)\n",
    "        self.proportion = image_height//patch_height  # 记录 patch 大小和图像大小之间的比例，以便后面进行转换\n",
    "        self.patch_size = patch_height\n",
    "        patch_dim = extract_dim*patch_height*patch_width   # 每个 patch 看成是一个 Token，其维度为 H'xW'xC\n",
    "        self.to_patch_embedding = nn.Sequential(    # 输入进行线性嵌入，似乎有点不一样？\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),  # 转为 Token\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim))  # 位置嵌入是可学习的？\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.feature_extractor = nn.Conv2d(in_channels=channels*2, out_channels=extract_dim, kernel_size=3, stride=1, padding=1)  # 使用 CNN 初步提取特征\n",
    "\n",
    "        self.to_latent = nn.Identity()    # 这是啥\n",
    "\n",
    "    def forward(self, img):\n",
    "        extract_feature = self.feature_extractor(img)\n",
    "        x = self.to_patch_embedding(extract_feature)\n",
    "        b, n, _ = x.shape\n",
    "        x += self.pos_embedding[:, :n]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)  # (B,H'xW',dim)\n",
    "\n",
    "        x = rearrange(x, 'b (h w) (p1 p2 c) -> b c (h p1) (w p2)', h=self.proportion, p1=self.patch_size, p2=self.patch_size)  # 转化为图像表示，(B,C,H,W)\n",
    "\n",
    "        return x\n",
    "\n",
    "class model1_decoder(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, dim, depth=1, heads, mlp_dim, channels=3, dim_head=64, dropout=0., emb_dropout=0.):\n",
    "        \"\"\"\n",
    "            image_size: 输入图像大小\n",
    "            patch_size: patch size\n",
    "            dim: token's length\n",
    "            extract_dim: feature extractor output's dim\n",
    "            depth: numbers of transformer block\n",
    "            heads: numbers of multi-attention head\n",
    "            mlp_dim: mlp's dim\n",
    "            dim_head: one of the head's dim\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'  # 防止划分不完全\n",
    "\n",
    "        num_patches = (image_height//patch_height)*(image_width//patch_width)\n",
    "        self.proportion = image_height//patch_height  # 记录 patch 大小和图像大小之间的比例，以便后面进行转换\n",
    "        self.patch_size = patch_height\n",
    "        patch_dim = channels*patch_height*patch_width   # 每个 patch 看成是一个 Token，其维度为 H'xW'xC\n",
    "        self.to_patch_embedding = nn.Sequential(    # 输入进行线性嵌入，似乎有点不一样？\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),  # 转为 Token\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim))  # 位置嵌入是可学习的？\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.to_latent = nn.Identity()    # 这是啥\n",
    "        \n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "        x += self.pos_embedding[:, :n]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)  # (B,H'xW',dim)\n",
    "\n",
    "        x = rearrange(x, 'b (h w) (p1 p2 c) -> b c (h p1) (w p2)', h=self.proportion, p1=self.patch_size, p2=self.patch_size)  # 转化为图像表示，(B,C,H,W)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test_model_encoder=model1_encoder(image_size=128,patch_size=16,dim=768,extract_dim=128,depth=6,heads=8,mlp_dim=1024,)   # dim 是 token 维度，dim=(patch_height x patch_width x channel)\n",
    "test_model_decoder=model1_decoder(image_size=128,patch_size=16,dim=768,depth=1,heads=8,mlp_dim=1024)\n",
    "input_data = torch.rand((1,6,128,128))\n",
    "output=test_model_encoder(input_data)\n",
    "output=test_model_decoder(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v0.2 模块化测试\n",
    "1. 双线模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stg_block(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, dim, heads, mlp_dim, channels=3, dim_head=64, dropout=0., emb_dropout=0.):\n",
    "        \"\"\"\n",
    "            image_size: 输入图像大小\n",
    "            patch_size: patch size\n",
    "            dim: token's length\n",
    "            heads: numbers of multi-attention head\n",
    "            mlp_dim: mlp's dim\n",
    "            dim_head: one of the head's dim\n",
    "            channels:每张图片的通道数！！！\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        num_patches = (image_height//patch_height)*(image_width//patch_width)\n",
    "        self.proportion = image_height//patch_height  # 记录 patch 大小和图像大小之间的比例，以便后面进行转换\n",
    "        self.patch_size = patch_height\n",
    "\n",
    "        self.transformer_block = nn.Sequential(\n",
    "            PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "            PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "        )\n",
    "        self.cnn_blcok = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2*channels, out_channels=4*channels, kernel_size=3, stride=2, padding=1),  # 特征通道数翻倍，分辨率减半\n",
    "            nn.BatchNorm2d(num_features=4*channels),\n",
    "            nn.ConvTranspose2d(in_channels=4*channels, out_channels=2*channels, kernel_size=3, stride=2, padding=1,output_padding=1)   # 恢复分辨率\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        input_img = img   # (B,2C,H,W)\n",
    "        img_token = rearrange(input_img, 'b (n c) (h p1) (w p2) -> b (n h w) (p1 p2 c)',n=2,p1=self.patch_size,p2=self.patch_size)  # 转为 Token（B,2N,L)\n",
    "        transformer_output=self.transformer_block(img_token)\n",
    "        transformer_output=rearrange(transformer_output,'b (n w) l -> b n w l',n=2)\n",
    "        transformer_output=rearrange(transformer_output,'b n (h w) (p1 p2 c) -> b (n c) (h p1) (w p2)', h=self.proportion,p1=self.patch_size,p2=self.patch_size)    # 转为图像表示\n",
    "\n",
    "        cnn_input = img\n",
    "        cnn_output = self.cnn_blcok(cnn_input)\n",
    "\n",
    "        res_output=cnn_output+transformer_output+img\n",
    "        return res_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**实例化测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          ...,\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "         [[True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          ...,\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "         [[True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          ...,\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          ...,\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "         [[True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          ...,\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "         [[True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          ...,\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True]]]])\n"
     ]
    }
   ],
   "source": [
    "stg_b=stg_block(image_size=128,patch_size=16,dim=2048,heads=8,mlp_dim=1024,channels=8,dim_head=256)\n",
    "input_test=torch.randn((1,16,128,128))\n",
    "test=input_test\n",
    "\n",
    "img_divide = rearrange(input_test, 'b (n c) (h p1) (w p2) -> b (n h w) (p1 p2 c)',n=2,p1=16,p2=16)  # 转为 Token（B,2N,L)\n",
    "img_divide=rearrange(img_divide,'b (n h w) (p1 p2 c) -> b (n c) (h p1) (w p2)', n=2,h=8,p1=16,p2=16)    # 转为图像表示\n",
    "print(img_divide == test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**实例化测试**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "test_model_encoder=model1_encoder(image_size=128,patch_size=16,dim=768,extract_dim=128,depth=6,heads=8,mlp_dim=1024,)   # dim 是 token 维度，dim=(patch_height x patch_width x channel)\n",
    "test_model_decoder=model1_decoder(image_size=128,patch_size=16,dim=768,depth=1,heads=8,mlp_dim=1024)\n",
    "input_data = torch.rand((1,6,128,128))\n",
    "output=test_model_encoder(input_data)\n",
    "output=test_model_decoder(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torchvision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用单张图片让模型过拟合以测试性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import math\n",
    "import numpy as np\n",
    "from model import *\n",
    "from tensorboardX import SummaryWriter\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomCrop(512),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "# 以类的方式定义参数\n",
    "class Args:\n",
    "    def __init__(self) -> None:\n",
    "        self.batch_size = 1\n",
    "        self.image_size = 256\n",
    "        self.patch_size = 16\n",
    "        self.lr = 1e-3\n",
    "        self.epochs = 10\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "args = Args()\n",
    "\n",
    "# 设置随机种子\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# 损失函数\n",
    "def steg_loss(img1,img2):\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True,size_average=False)\n",
    "    loss = loss_fn(img1,img2)\n",
    "    return loss.to(args.device)\n",
    "\n",
    "def reconstruction_loss(img1,img2):\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True,size_average=False)\n",
    "    loss = loss_fn(img1,img2)\n",
    "    return loss.to(args.device)\n",
    "\n",
    "# tensorboard\n",
    "writer = SummaryWriter(log_dir='logs')\n",
    "\n",
    "# 模型初始化\n",
    "encoder = model1_encoder(image_size=512, patch_size=16, dim=768, extract_dim=128, depth=6, heads=8, mlp_dim=1024,)   # dim 是 token 维度，dim=(patch_height x patch_width x channel)\n",
    "decoder = model1_decoder(image_size=512, patch_size=16, dim=768, depth=1, heads=8, mlp_dim=1024)\n",
    "encoder.cuda()\n",
    "decoder.cuda()\n",
    "\n",
    "# 用于过拟合的两张图像\n",
    "cover=Image.open(\"F:/dataset/test/cover/0802.png\").convert('RGB')\n",
    "secret=Image.open(\"F:/dataset/test/secret/0801.png\").convert('RGB')\n",
    "cover=transform(cover)\n",
    "secret=transform(secret)\n",
    "cover=torch.unsqueeze(cover,0)\n",
    "secret=torch.unsqueeze(secret,0)\n",
    "\n",
    "# 优化器\n",
    "optim = torch.optim.AdamW([{'params': encoder.parameters()}, {'params': decoder.parameters()}], lr=args.lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用单张图片进行过拟合测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\deep_learning\\dl_env\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode_loss:2088243.75, recontruct_loss: 1855798.0\n",
      "encode_loss:2092841.0, recontruct_loss: 1811178.75\n",
      "encode_loss:1528301.0, recontruct_loss: 1750252.25\n",
      "encode_loss:1334093.5, recontruct_loss: 1693350.75\n",
      "encode_loss:1148063.0, recontruct_loss: 1626662.75\n",
      "encode_loss:1072494.125, recontruct_loss: 1539712.5\n",
      "encode_loss:980572.9375, recontruct_loss: 1453182.75\n",
      "encode_loss:929544.3125, recontruct_loss: 1384761.25\n",
      "encode_loss:884019.5625, recontruct_loss: 1317560.75\n",
      "encode_loss:858181.375, recontruct_loss: 1254858.25\n",
      "encode_loss:944178.5, recontruct_loss: 1295881.75\n",
      "encode_loss:875073.375, recontruct_loss: 1251150.75\n",
      "encode_loss:839129.875, recontruct_loss: 1054337.25\n",
      "encode_loss:819080.875, recontruct_loss: 952952.375\n",
      "encode_loss:793683.1875, recontruct_loss: 879189.1875\n",
      "encode_loss:744834.5625, recontruct_loss: 847020.8125\n",
      "encode_loss:715429.25, recontruct_loss: 809703.625\n",
      "encode_loss:677606.875, recontruct_loss: 783306.875\n",
      "encode_loss:647172.75, recontruct_loss: 755442.75\n",
      "encode_loss:615952.5, recontruct_loss: 734353.75\n",
      "encode_loss:590134.5625, recontruct_loss: 712128.25\n",
      "encode_loss:563504.875, recontruct_loss: 695341.5\n",
      "encode_loss:536455.625, recontruct_loss: 700835.4375\n",
      "encode_loss:528329.125, recontruct_loss: 692531.75\n",
      "encode_loss:490338.96875, recontruct_loss: 667410.6875\n",
      "encode_loss:546683.3125, recontruct_loss: 632691.0\n",
      "encode_loss:594115.0625, recontruct_loss: 746338.0\n",
      "encode_loss:674889.625, recontruct_loss: 636687.4375\n",
      "encode_loss:902644.625, recontruct_loss: 1450422.0\n",
      "encode_loss:1217799.5, recontruct_loss: 750300.1875\n",
      "encode_loss:926166.375, recontruct_loss: 644681.625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m r_loss\u001b[39m=\u001b[39mreconstruction_loss(secret\u001b[39m.\u001b[39mcuda(),decode_img\u001b[39m.\u001b[39mcuda())\n\u001b[0;32m     20\u001b[0m total_loss\u001b[39m=\u001b[39mh_loss\u001b[39m+\u001b[39mr_loss\n\u001b[1;32m---> 21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mencode_loss:\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(h_loss\u001b[39m.\u001b[39;49mitem())\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, recontruct_loss: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(r_loss\u001b[39m.\u001b[39mitem()))\n\u001b[0;32m     23\u001b[0m total_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     24\u001b[0m optim\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 过拟合代码\n",
    "for i_epooch in range(3000):\n",
    "    cover = cover.to(args.device)\n",
    "    secret = secret.to(args.device)\n",
    "    input_img = torch.cat((cover, secret), 1)\n",
    "\n",
    "    # encode\n",
    "    encode_img = encoder(input_img)\n",
    "    \n",
    "    # decode\n",
    "    decode_img = decoder(encode_img)\n",
    "\n",
    "    if(i_epooch%100==0):\n",
    "        torchvision.utils.save_image(encode_img,\"encode_img_\"+str(i_epooch)+\".png\",normalize=True)\n",
    "        torchvision.utils.save_image(decode_img,\"decode_img_\"+str(i_epooch)+\".png\",normalize=True)\n",
    "\n",
    "\n",
    "    h_loss=steg_loss(cover.cuda(),encode_img.cuda())\n",
    "    r_loss=reconstruction_loss(secret.cuda(),decode_img.cuda())\n",
    "    total_loss=h_loss+r_loss\n",
    "    print(\"encode_loss:\"+str(h_loss.item())+\", recontruct_loss: \"+str(r_loss.item()))\n",
    "\n",
    "    total_loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用训练集进行训练测试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练代码\n",
    "for i_epooch in range(3000):\n",
    "    for i_batch, (cover, secret) in enumerate(zip(datasets.DIV2K_train_cover_loader, datasets.DIV2K_train_secret_loader)) :\n",
    "        cover = cover.to(args.device)\n",
    "        secret = secret.to(args.device)\n",
    "        input_img = torch.cat((cover, secret), 1)\n",
    "        input_img=input_img.to(args.device)\n",
    "        # print(input_img.shape)\n",
    "\n",
    "        # encode\n",
    "        encode_img = encoder(input_img)\n",
    "        \n",
    "        # decode\n",
    "        decode_img = decoder(encode_img)\n",
    "\n",
    "        h_loss=steg_loss(cover.cuda(),encode_img.cuda())\n",
    "        r_loss=reconstruction_loss(secret.cuda(),decode_img.cuda())\n",
    "        total_loss=h_loss+r_loss\n",
    "        if(i_batch==0):\n",
    "            print(total_loss)\n",
    "\n",
    "        total_loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.metrics import *\n",
    "\n",
    "def calculate_ssim(img1,img2):\n",
    "    img1=np.array(img1).astype(np.float64)*255\n",
    "    img2=np.array(img2).astype(np.float64)*255\n",
    "    img1=np.clip(img1,0,255)\n",
    "    img2=np.clip(img2,0,255)\n",
    "    ssim_score=structural_similarity(img1,img2,channel_axis=1)\n",
    "    return ssim_score\n",
    "    \n",
    "\n",
    "def calculate_psnr(img1, img2):\n",
    "    # 转为 float64 防止精度丢失\n",
    "    test1=np.array(img1).astype(np.float64)\n",
    "    test2=np.array(img2).astype(np.float64)\n",
    "    return peak_signal_noise_ratio(test1,test2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.781162383004529"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "test1=torch.rand((16,3,16,16))\n",
    "test2=torch.rand((16,3,16,16))\n",
    "\n",
    "calculate_psnr(test1,test2)\n",
    "# calculate_ssim(test1,test1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff27b36bd867d4973fa82b6d8489c0c4f95b325b9f3405a0982e94b5e84e7a9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
